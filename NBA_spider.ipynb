{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA球员数据 爬虫+数据固化+可视化 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于腾讯体育网页采用异步加载，选择selenium作为爬虫方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.421286Z",
     "start_time": "2021-03-14T11:23:50.599151Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pymysql\n",
    "from urllib.request import urlretrieve\n",
    "import requests\n",
    "import os \n",
    "import time\n",
    "import threading\n",
    "from selenium import webdriver  \n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument('--headless')  # 无头模式 不显示浏览器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T10:40:55.995993Z",
     "start_time": "2021-03-13T10:40:55.734142Z"
    }
   },
   "source": [
    "## 爬取与解析 selenium+xpath+多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照30支球队，解析球队信息\n",
    "按照球员解析个人信息\n",
    "每将数据固化到csv文件，共计应产生1个联盟球队信息文件及30个球队球员信息文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.440256Z",
     "start_time": "2021-03-14T11:23:51.423281Z"
    }
   },
   "outputs": [],
   "source": [
    "def team_parse(tid, df):\n",
    "    \n",
    "    try:\n",
    "        browser = webdriver.Chrome(options=chrome_options)\n",
    "        browser.get('https://sports.qq.com/kbsweb/teams.htm?tid={0}&cid=100000#stats'.format(tid))\n",
    "        browser.maximize_window()\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    team_name = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[1]/div')\n",
    "    team_pic_url = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/img')\n",
    "    area = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[1]/ul/li[1]/span[1]')\n",
    "    coach = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[1]/ul/li[3]/span[2]')\n",
    "    home_court = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[1]/ul/li[4]/span[2]')\n",
    "    win_num = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[2]/div[1]/span[1]/em')\n",
    "    lose_num = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[2]/div[1]/span[2]/em')\n",
    "    score = browser.find_elements_by_xpath('//*[@id=\"union-contrast\"]/div/div[2]/div[2]/div[1]/div[1]/span')\n",
    "    rebound = browser.find_elements_by_xpath('//*[@id=\"union-contrast\"]/div/div[2]/div[2]/div[2]/div[1]/span')\n",
    "    assist = browser.find_elements_by_xpath('//*[@id=\"union-contrast\"]/div/div[2]/div[2]/div[3]/div[1]/span')\n",
    "    steal = browser.find_elements_by_xpath('//*[@id=\"union-contrast\"]/div/div[2]/div[2]/div[4]/div[1]/span')\n",
    "    block = browser.find_elements_by_xpath('//*[@id=\"union-contrast\"]/div/div[2]/div[2]/div[5]/div[1]/span')\n",
    "    \n",
    "    df.loc[tid, 'team_name'] = [i.text for i in team_name][0]\n",
    "    df.loc[tid, 'team_pic_url'] = [i.get_attribute('src') for i in team_pic_url][0]\n",
    "    df.loc[tid, 'area'] = [i.text for i in area][0]\n",
    "    df.loc[tid, 'coach'] = [i.text for i in coach][0]\n",
    "    df.loc[tid, 'home_court'] = [i.text for i in home_court][0]\n",
    "    df.loc[tid, 'win_num'] = [i.text for i in win_num][0]\n",
    "    df.loc[tid, 'lose_num'] = [i.text for i in lose_num][0]\n",
    "    df.loc[tid, 'score'] = [i.text for i in score][0]\n",
    "    df.loc[tid, 'rebound'] = [i.text for i in rebound][0]\n",
    "    df.loc[tid, 'assist'] = [i.text for i in assist][0]\n",
    "    df.loc[tid, 'steal'] = [i.text for i in steal][0]\n",
    "    df.loc[tid, 'block'] = [i.text for i in block][0]\n",
    "    \n",
    "    browser.quit()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.473239Z",
     "start_time": "2021-03-14T11:23:51.445253Z"
    }
   },
   "outputs": [],
   "source": [
    "def player_parse(tid):\n",
    "    \n",
    "    try:\n",
    "        browser = webdriver.Chrome(options=chrome_options)\n",
    "        browser.get('https://sports.qq.com/kbsweb/teams.htm?tid={0}&cid=100000#roster'.format(tid))\n",
    "        browser.maximize_window()\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    df = pd.DataFrame()\n",
    "    lent = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody')\n",
    "    player_num = [i.text for i in lent][0].count('\\n') + 1  # 通过统计球员标签换了多少行来统计有多少人\n",
    "\n",
    "    for i in range(player_num):\n",
    "\n",
    "        player_pic_url = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[1]/a/img'.format(i+1))\n",
    "        player_name = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[1]/a/span'.format(i+1))\n",
    "        player_number = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[2]/span'.format(i+1))\n",
    "        player_roles = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[3]/span'.format(i+1))\n",
    "        player_height = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[4]/span'.format(i+1))\n",
    "        player_weight = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[5]/span'.format(i+1))\n",
    "        play_years = browser.find_elements_by_xpath('//*[@id=\"teamContent\"]/div[3]/div[3]/div/table/tbody/tr[{0}]/td[6]/span'.format(i+1))\n",
    "\n",
    "\n",
    "        df.loc[i, 'player_pic_url'] = [i.get_attribute('src') for i in player_pic_url][0]\n",
    "        df.loc[i, 'player_name'] = [i.text for i in player_name][0]\n",
    "        df.loc[i, 'player_number'] = [i.text for i in player_number][0]\n",
    "        df.loc[i, 'player_roles'] = [i.text for i in player_roles][0]\n",
    "        df.loc[i, 'player_height'] = [i.text for i in player_height][0]\n",
    "        df.loc[i, 'player_weight'] = [i.text for i in player_weight][0]\n",
    "        df.loc[i, 'play_years'] = [i.text for i in play_years][0]\n",
    "        \n",
    "    team_name = browser.find_elements_by_xpath('//*[@id=\"team-introduction\"]/div/div/div[1]/div')\n",
    "    df.to_csv('players_{0}.csv'.format([i.text for i in team_name][0]), encoding='utf-8-sig')\n",
    "    print('-'*20 + '第{0}支球队全体球员数据已写入csv文件'.format(tid) + '-'*20)\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T16:58:10.970808Z",
     "start_time": "2021-03-13T16:58:10.637999Z"
    }
   },
   "source": [
    "调用爬取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.489228Z",
     "start_time": "2021-03-14T11:23:51.476235Z"
    }
   },
   "outputs": [],
   "source": [
    "# for team_id in range(24, 31):\n",
    "#     team_df = pd.DataFrame()\n",
    "#     print('-'*20 + '正在下载第{0}支球队综合数据'.format(team_id) + '-'*20)\n",
    "#     team_parse(team_id, team_df).to_csv('teams_联盟_1.csv',mode='a',encoding='utf-8-sig')\n",
    "#     print('-'*20 + '第{0}支球队综合数据已写入csv文件'.format(team_id) + '-'*20)\n",
    "    \n",
    "#     print('-'*20 + '正在下载第{0}支球队球员数据'.format(team_id) + '-'*20)\n",
    "#     player_parse(team_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多线程爬取 - 更快更高效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.520211Z",
     "start_time": "2021-03-14T11:23:51.492226Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部爬取完毕.\n"
     ]
    }
   ],
   "source": [
    "def multi_parse(start_id, end_id):\n",
    "    for team_id in range(start_id, end_id):\n",
    "\n",
    "        team_df = pd.DataFrame()\n",
    "        print('-'*20 + '正在下载第{0}支球队综合数据'.format(team_id) + '-'*20)\n",
    "        team_parse(team_id, team_df).to_csv('teams_multi.csv',mode='a',encoding='utf-8-sig')\n",
    "        print('-'*20 + '第{0}支球队综合数据已写入csv文件'.format(team_id) + '-'*20)\n",
    "\n",
    "        print('-'*20 + '正在下载第{0}支球队球员数据'.format(team_id) + '-'*20)\n",
    "        print(team_id)\n",
    "        player_parse(team_id)\n",
    "\n",
    "\n",
    "threads=[]\n",
    " \n",
    "t1 = threading.Thread(target=multi_parse,args=(1,8))\n",
    "threads.append(t1)\n",
    " \n",
    "t2 = threading.Thread(target=multi_parse,args=(8,16))\n",
    "threads.append(t2)\n",
    " \n",
    "t3 = threading.Thread(target=multi_parse,args=(16,24))\n",
    "threads.append(t3)\n",
    " \n",
    "t4 = threading.Thread(target=multi_parse,args=(24,31))\n",
    "threads.append(t4)\n",
    "\n",
    "# join() 这会阻塞调用线程，直到其join（）方法被调用的线程终止 - 此处可以让主程序挂起，以正常打印日志\n",
    "\n",
    "# for t in threads:\n",
    "#     t.start()\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "    \n",
    "print('全部爬取完毕.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取结束，数据已被存储到csv文件中，下一步运用pymysql将数据批量固化至mysql数据库中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据固化 pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.558191Z",
     "start_time": "2021-03-14T11:23:51.527206Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['players_76人.csv',\n",
       " 'players_公牛.csv',\n",
       " 'players_凯尔特人.csv',\n",
       " 'players_勇士.csv',\n",
       " 'players_国王.csv',\n",
       " 'players_太阳.csv',\n",
       " 'players_奇才.csv',\n",
       " 'players_尼克斯.csv',\n",
       " 'players_开拓者.csv',\n",
       " 'players_快船.csv',\n",
       " 'players_掘金.csv',\n",
       " 'players_森林狼.csv',\n",
       " 'players_步行者.csv',\n",
       " 'players_活塞.csv',\n",
       " 'players_湖人.csv',\n",
       " 'players_火箭.csv',\n",
       " 'players_灰熊.csv',\n",
       " 'players_热火.csv',\n",
       " 'players_爵士.csv',\n",
       " 'players_独行侠.csv',\n",
       " 'players_猛龙.csv',\n",
       " 'players_篮网.csv',\n",
       " 'players_老鹰.csv',\n",
       " 'players_雄鹿.csv',\n",
       " 'players_雷霆.csv',\n",
       " 'players_马刺.csv',\n",
       " 'players_骑士.csv',\n",
       " 'players_魔术.csv',\n",
       " 'players_鹈鹕.csv',\n",
       " 'players_黄蜂.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = r'C:\\Users\\Jianming\\Desktop\\技术栈\\爬虫\\NBA_spider\\爬取结果\\players'\n",
    "filelist = os.listdir(datadir)\n",
    "filelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先建立NBA数据库用于存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.585174Z",
     "start_time": "2021-03-14T11:23:51.563186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已创建数据库 NBA_db\n"
     ]
    }
   ],
   "source": [
    "conn = pymysql.connect(user='root', passwd='xjm970722', db='test_db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('CREATE DATABASE IF NOT EXISTS NBA_db DEFAULT CHARSET utf8 COLLATE utf8_general_ci;')\n",
    "cursor.close()  # 先关闭游标\n",
    "conn.close()  # 再关闭数据库连接\n",
    "print('已创建数据库 NBA_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量创建数据表的函数构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.652135Z",
     "start_time": "2021-03-14T11:23:51.588172Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_table(table):\n",
    "\n",
    "    conn = pymysql.connect(user='root', passwd='xjm970722', db='NBA_db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('drop table if exists {0}'.format(table))\n",
    "    sql = \"\"\"CREATE TABLE IF NOT EXISTS `{0}`(\n",
    "                `player_name` varchar(255) NOT NULL,\n",
    "                `player_pic_url` varchar(255) NOT NULL,\n",
    "                `player_number` int(11) NOT NULL,\n",
    "                `player_roles` varchar(255) NOT NULL,\n",
    "                `player_weight` float NOT NULL,\n",
    "                `player_height` float NOT NULL,\n",
    "                `play_years` int(11),\n",
    "                PRIMARY KEY(`player_name`)\n",
    "            ) ENGINE=InnoDB  DEFAULT CHARSET=utf8 AUTO_INCREMENT=0\n",
    "            \"\"\".format(table)\n",
    "\n",
    "    cursor.execute(sql)\n",
    "    print('创建数据表: {0}'.format(table))\n",
    "\n",
    "    data = pd.read_csv(r'C:\\Users\\Jianming\\Desktop\\技术栈\\爬虫\\NBA_spider\\爬取结果\\players\\{0}.csv'.format(table)).iloc[:, 1:]\n",
    "    data.play_years.replace('-', '0', inplace = True)\n",
    "    data_insert = data.values.tolist()  # 准备数据，即将插入数据表\n",
    "\n",
    "    cursor.executemany('INSERT INTO {0} VALUES(%s, %s, %s, %s, %s, %s, %s)'.format(table), data_insert)\n",
    "    \n",
    "    print('已插入数据到: {0}'.format(table))\n",
    "\n",
    "    cursor.close()\n",
    "    conn.commit()  # 将更改保存到数据库\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量由csv插入数据库表\n",
    "(联盟球队综合只有一张表，手动导入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:51.675122Z",
     "start_time": "2021-03-14T11:23:51.659130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for table in filelist:\n",
    "#     table = table.split('.')[0]\n",
    "#     insert_table(table)\n",
    "# print('球员数据全部存储完毕')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现需要构建一张包括所有球员的大表，同时新增一列 player_team 表示所属球队 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T12:04:37.548594Z",
     "start_time": "2021-03-14T12:04:37.541597Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_table_total(table):\n",
    "\n",
    "    conn = pymysql.connect(user='root', passwd='xjm970722', db='nba_db')\n",
    "    cursor = conn.cursor()\n",
    "#     cursor.execute('drop table if exists total_players')\n",
    "    sql = \"\"\"CREATE TABLE IF NOT EXISTS `total_players`(\n",
    "                `player_pic_url` varchar(255) NOT NULL,\n",
    "                `player_name` varchar(255) NOT NULL,\n",
    "                `player_number` int(11) NOT NULL,\n",
    "                `player_roles` varchar(255) NOT NULL,\n",
    "                `player_weight` float NOT NULL,\n",
    "                `player_height` float NOT NULL,\n",
    "                `play_years` int(11),\n",
    "                `player_team` varchar(255) NOT NULL\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    data = pd.read_csv(r'C:\\Users\\Jianming\\Desktop\\技术栈\\爬虫\\NBA_spider\\爬取结果\\players\\{0}.csv'.format(table)).iloc[:, 1:]\n",
    "    data.play_years.replace('-', '0', inplace = True)\n",
    "    data.loc[:, 'player_team'] = table.split('_')[1]\n",
    "    data_insert = data.values.tolist()  # 准备数据，即将插入数据表\n",
    "\n",
    "    cursor.executemany('INSERT INTO total_players VALUES(%s, %s, %s, %s, %s, %s, %s, %s)', data_insert)\n",
    "    \n",
    "    print('已插入{0}数据到total_players'.format(table))\n",
    "\n",
    "    cursor.close()\n",
    "    conn.commit()  # 将更改保存到数据库\n",
    "    conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T12:04:37.548594Z",
     "start_time": "2021-03-14T12:04:37.541597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员数据大表生成完毕\n"
     ]
    }
   ],
   "source": [
    "# for table in filelist:\n",
    "#     table = table.split('.')[0]\n",
    "#     insert_table_total(table)\n",
    "# print('球员数据大表生成完毕')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化分析 Tableau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以根据我们之前爬取的球队队标，team_pic_url下载对应图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T11:23:54.893279Z",
     "start_time": "2021-03-14T11:23:54.889276Z"
    }
   },
   "outputs": [],
   "source": [
    "# url_list = pd.read_csv(r'C:/Users/Jianming/Desktop/技术栈/爬虫/NBA_spider/爬取结果/teams/teams_联盟.csv').team_pic_url.to_list()\n",
    "# os.makedirs('./爬取结果/pics/', exist_ok=True)\n",
    "\n",
    "# i = 1\n",
    "# for url in url_list:\n",
    "#     urlretrieve(url, './爬取结果/pics/pic_{0}.png'.format(i))\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理下载球员图片\n",
    "此处不需要全部下载"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
